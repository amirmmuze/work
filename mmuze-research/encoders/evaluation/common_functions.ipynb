{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def sim_exact_cross(embs1, embs2):\n",
    "    input1 = tf.placeholder(tf.float64, shape=(None))\n",
    "    input2 = tf.placeholder(tf.float64, shape=(None))\n",
    "\n",
    "    norm1 = tf.nn.l2_normalize(input1, axis=1)\n",
    "    norm2 = tf.nn.l2_normalize(input2, axis=1)\n",
    "    cosine_similarities = tf.tensordot(norm1, norm2, axes=((1),(1)))\n",
    "#     cosine_similarities = tf.reduce_sum(vectorsdot, axis=1)\n",
    "    clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
    "    angular_dist = 1.0 - tf.acos(clip_cosine_similarities)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "  \n",
    "        norm1, norm2, angdist, cossim = session.run([norm1, norm2, angular_dist, cosine_similarities], \n",
    "                                         feed_dict={ input1: embs1,\n",
    "                                                     input2: embs2 })\n",
    "          \n",
    "    return angdist, cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def sim_exact(embs1, embs2):\n",
    "    input1 = tf.placeholder(tf.float64, shape=(None))\n",
    "    input2 = tf.placeholder(tf.float64, shape=(None))\n",
    "\n",
    "    norm1 = tf.nn.l2_normalize(input1, axis=1)\n",
    "    norm2 = tf.nn.l2_normalize(input2, axis=1)\n",
    "    cosine_similarities = tf.reduce_sum(tf.multiply(norm1, norm2), 1, keepdims=True)\n",
    "#     cosine_similarities = tf.reduce_sum(tf.multiply(norm1, norm2,), axis=1)    \n",
    "#     cosine_similarities = tf.diag_part(tf.tensordot(norm1, norm2, axes=((1),(1))))   \n",
    "    clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)    \n",
    "    angular_dist = 1.0 - tf.acos(clip_cosine_similarities)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "  \n",
    "        norm1, norm2, angdist, cossim = session.run([norm1, norm2, angular_dist, cosine_similarities], \n",
    "                                         feed_dict={ input1: embs1,\n",
    "                                                     input2: embs2 })\n",
    "          \n",
    "    return angdist, cossim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plot_similarity(labels, corr): \n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(labels, rotation=90)\n",
    "    g.set_title(\"Semantic Textual Similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve,average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.fixes import signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(y_score, y_true, display_plot=True):\n",
    "    from sklearn.metrics import precision_recall_curve,average_precision_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.utils.fixes import signature\n",
    "    # y_true : array, shape = [n_samples] or [n_samples, n_classes] True binary labels or binary label indicators.\n",
    "    # y_score : array, shape = [n_samples] or [n_samples, n_classes]  Target scores, can either be probability estimates of the positive\n",
    "    # class, confidence values, or non-thresholded measure of decisions   (as returned by \"decision_function\" on some classifiers).\n",
    "    average_precision = average_precision_score(y_true, y_score)\n",
    "    # y_true : array, shape = [n_samples]  True targets of binary classification in range {-1, 1} or {0, 1}.\n",
    "    # probas_pred : array, shape = [n_samples]  Estimated probabilities or decision function.\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "    if display_plot:\n",
    "        # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "        step_kwargs = ({'step': 'post'}\n",
    "                       if 'step' in signature(plt.fill_between).parameters\n",
    "                       else {})\n",
    "        plt.step(recall, precision, color='b', alpha=0.2,\n",
    "                 where='post')\n",
    "        plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "    \n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_curve(y_score, y_true, display_plot=True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    # Compute ROC curve and area the curve\n",
    "    # y_true : array, shape = [n_samples] True binary labels. If labels are not either {-1, 1} or {0, 1}, then pos_label should be explicitly given.\n",
    "    # y_score : array, shape = [n_samples] Target scores, can either be probability estimates of the positive class, confidence values, \n",
    "    # or non-thresholded measure of decisions (as returned by \"decision_function\" on some classifiers).\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    if display_plot:\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC (area = %0.2f)' % (roc_auc))\n",
    "        plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_by_chunks(df_big, chunk_size, chunk_delegate):  \n",
    "    df_chunks = [df_big[i:i+chunk_size] for i in range(0,df_big.shape[0],chunk_size)]    \n",
    "    df_results = []\n",
    "    for df_chunk in df_chunks:\n",
    "        df_chunk = chunk_delegate(df_chunk)       \n",
    "        df_results.append(df_chunk)\n",
    "    df_res = pd.concat(df_results)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def df_crossjoin(df1, df2, **kwargs):\n",
    "    df1['_tmpkey'] = 1\n",
    "    df2['_tmpkey'] = 1\n",
    "\n",
    "    res = pd.merge(df1, df2, on='_tmpkey', **kwargs).drop('_tmpkey', axis=1)\n",
    "    res.index = pd.MultiIndex.from_product((df1.index, df2.index))\n",
    "\n",
    "    df1.drop('_tmpkey', axis=1, inplace=True)\n",
    "    df2.drop('_tmpkey', axis=1, inplace=True)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_by_chunks(collection, chunksize=1, start_from=0, query={}, projection={}):\n",
    "    chunks = range(start_from, collection.find(query).count(), int(chunksize))\n",
    "    num_chunks = len(chunks)\n",
    "    for i in range(1,num_chunks+1):\n",
    "        if i < num_chunks:\n",
    "            yield collection.find(query, projection=projection)[chunks[i-1]:chunks[i]]\n",
    "        else:\n",
    "            yield collection.find(query, projection=projection)[chunks[i-1]:chunks.stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
