{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import modeling\n",
    "import tokenization\n",
    "import tensorflow as tf\n",
    "import easydict \n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class PoolingStrategy(Enum):\n",
    "    NONE = 0\n",
    "    REDUCE_MAX = 1\n",
    "    REDUCE_MEAN = 2\n",
    "    REDUCE_MEAN_MAX = 3\n",
    "    FIRST_TOKEN = 4  # corresponds to [CLS] for single sequences\n",
    "    LAST_TOKEN = 5  # corresponds to [SEP] for single sequences\n",
    "    CLS_TOKEN = 4  # corresponds to the first token for single seq.\n",
    "    SEP_TOKEN = 5  # corresponds to the last token for single seq.\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    @staticmethod\n",
    "    def from_string(s):\n",
    "        try:\n",
    "            return PoolingStrategy[s]\n",
    "        except KeyError:\n",
    "            raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_BASE_DIR='./uncased_L-24_H-1024_A-16/'\n",
    "# bert_config_path=BERT_BASE_DIR + \"bert_config.json\"\n",
    "# vocab_file= \n",
    "# init_checkpoint=BERT_BASE_DIR+\"bert_model.ckpt\"\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"model_dir\": BERT_BASE_DIR,\n",
    "    'tuned_model_dir': None,\n",
    "    'ckpt_name': 'bert_model.ckpt',\n",
    "    'config_name': 'bert_config.json',\n",
    "    'max_seq_len': 128,\n",
    "    'xla': False,\n",
    "    'pooling_strategy': PoolingStrategy.REDUCE_MEAN,\n",
    "    'pooling_layer': [-2],\n",
    "    'do_lower_case': True,\n",
    "    'vocab_file': BERT_BASE_DIR+\"vocab.txt\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, unique_id, text_a, text_b):\n",
    "        self.unique_id = unique_id\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"unique_id:{self.unique_id}; text_a:{self.text_a}; text_b:{self.text_b}\"\n",
    "    __repr__ = __str__\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"unique_id:{self.unique_id}; \\\n",
    "                 tokens:{self.tokens}; \\\n",
    "                 input_type_ids:{self.input_type_ids} \\\n",
    "                 input_ids:{self.input_ids} \\\n",
    "                 input_mask:{self.input_mask}\"\n",
    "    __repr__ = __str__\n",
    "    \n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "          break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "          tokens_a.pop()\n",
    "        else:\n",
    "          tokens_b.pop()\n",
    "    \n",
    "def convert_examples_to_features(examples, seq_length, tokenizer):\n",
    "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "  features = []\n",
    "  for (ex_index, example) in enumerate(examples):\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "      tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "      # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "      # length is less than the specified length.\n",
    "      # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "      _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
    "    else:\n",
    "      # Account for [CLS] and [SEP] with \"- 2\"\n",
    "      if len(tokens_a) > seq_length - 2:\n",
    "        tokens_a = tokens_a[0:(seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "      tokens.append(token)\n",
    "      input_type_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "      for token in tokens_b:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(1)\n",
    "      tokens.append(\"[SEP]\")\n",
    "      input_type_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "      input_type_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == seq_length\n",
    "    assert len(input_mask) == seq_length\n",
    "    assert len(input_type_ids) == seq_length\n",
    "\n",
    "    if ex_index < 1:\n",
    "      tf.logging.info(\"*** Example ***\")\n",
    "      tf.logging.info(\"unique_id: %s\" % (example.unique_id))\n",
    "      tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "          [tokenization.printable_text(x) for x in tokens]))\n",
    "      tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "      tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "      tf.logging.info(\n",
    "          \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "\n",
    "    features.append(\n",
    "        InputFeatures(\n",
    "            unique_id=example.unique_id,\n",
    "            tokens=tokens,\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            input_type_ids=input_type_ids))\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "import modeling\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import contextlib\n",
    "import tempfile\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "class BertEstimator():\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.args = args        \n",
    "        self.logger = logger#logging.getLogger()\n",
    "        self.graph_path = self.optimize_graph(args, self.logger)\n",
    "    \n",
    "    def optimize_graph(self, args, logger):\n",
    "        print(\"SASHA: optimize_graph\")\n",
    "        try:\n",
    "            # we don't need GPU for optimizing the graph        \n",
    "            from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n",
    "\n",
    "            config = tf.ConfigProto(device_count={'GPU': 0}, allow_soft_placement=True)\n",
    "\n",
    "            config_fp = os.path.join(args.model_dir, args.config_name)\n",
    "            init_checkpoint = os.path.join(args.tuned_model_dir or args.model_dir, args.ckpt_name)\n",
    "            print(f\"SASHA: init_checkpoint: {init_checkpoint}\")\n",
    "            logger.info('model config: %s' % config_fp)\n",
    "            logger.info(\n",
    "                'checkpoint%s: %s' % (' (override by fine-tuned model)' if args.tuned_model_dir else '', init_checkpoint))\n",
    "            with tf.gfile.GFile(config_fp, 'r') as f:\n",
    "                bert_config = modeling.BertConfig.from_dict(json.load(f))\n",
    "\n",
    "            logger.info('build graph...')\n",
    "            # input placeholders, not sure if they are friendly to XLA\n",
    "            input_ids = tf.placeholder(tf.int32, (None, args.max_seq_len), 'input_ids')\n",
    "            input_mask = tf.placeholder(tf.int32, (None, args.max_seq_len), 'input_mask')\n",
    "            input_type_ids = tf.placeholder(tf.int32, (None, args.max_seq_len), 'input_type_ids')\n",
    "\n",
    "            jit_scope = tf.contrib.compiler.jit.experimental_jit_scope if args.xla else contextlib.suppress\n",
    "\n",
    "            with jit_scope():\n",
    "                input_tensors = [input_ids, input_mask, input_type_ids]\n",
    "\n",
    "                model = modeling.BertModel(\n",
    "                    config=bert_config,\n",
    "                    is_training=False,\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    token_type_ids=input_type_ids,\n",
    "                    use_one_hot_embeddings=False)\n",
    "\n",
    "                tvars = tf.trainable_variables()\n",
    "\n",
    "                (assignment_map, initialized_variable_names\n",
    "                 ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "\n",
    "                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "                minus_mask = lambda x, m: x - tf.expand_dims(1.0 - m, axis=-1) * 1e30\n",
    "                mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "                masked_reduce_max = lambda x, m: tf.reduce_max(minus_mask(x, m), axis=1)\n",
    "                masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                        tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "\n",
    "                with tf.variable_scope(\"pooling\"):\n",
    "                    if len(args.pooling_layer) == 1:\n",
    "                        encoder_layer = model.all_encoder_layers[args.pooling_layer[0]]\n",
    "                    else:\n",
    "                        all_layers = [model.all_encoder_layers[l] for l in args.pooling_layer]\n",
    "                        encoder_layer = tf.concat(all_layers, -1)\n",
    "\n",
    "                    input_mask = tf.cast(input_mask, tf.float32)\n",
    "                    if args.pooling_strategy == PoolingStrategy.REDUCE_MEAN:\n",
    "                        pooled = masked_reduce_mean(encoder_layer, input_mask)\n",
    "                    elif args.pooling_strategy == PoolingStrategy.REDUCE_MAX:\n",
    "                        pooled = masked_reduce_max(encoder_layer, input_mask)\n",
    "                    elif args.pooling_strategy == PoolingStrategy.REDUCE_MEAN_MAX:\n",
    "                        pooled = tf.concat([masked_reduce_mean(encoder_layer, input_mask),\n",
    "                                            masked_reduce_max(encoder_layer, input_mask)], axis=1)\n",
    "                    elif args.pooling_strategy == PoolingStrategy.FIRST_TOKEN or \\\n",
    "                            args.pooling_strategy == PoolingStrategy.CLS_TOKEN:\n",
    "                        pooled = tf.squeeze(encoder_layer[:, 0:1, :], axis=1)\n",
    "                    elif args.pooling_strategy == PoolingStrategy.LAST_TOKEN or \\\n",
    "                            args.pooling_strategy == PoolingStrategy.SEP_TOKEN:\n",
    "                        seq_len = tf.cast(tf.reduce_sum(input_mask, axis=1), tf.int32)\n",
    "                        rng = tf.range(0, tf.shape(seq_len)[0])\n",
    "                        indexes = tf.stack([rng, seq_len - 1], 1)\n",
    "                        pooled = tf.gather_nd(encoder_layer, indexes)\n",
    "                    elif args.pooling_strategy == PoolingStrategy.NONE:\n",
    "                        pooled = mul_mask(encoder_layer, input_mask)\n",
    "                    else:\n",
    "                        raise NotImplementedError()\n",
    "\n",
    "                pooled = tf.identity(pooled, 'final_encodes')\n",
    "\n",
    "                output_tensors = [pooled]\n",
    "                tmp_g = tf.get_default_graph().as_graph_def()\n",
    "\n",
    "            with tf.Session(config=config) as sess:\n",
    "                logger.info('load parameters from checkpoint...')\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                logger.info('freeze...')\n",
    "                tmp_g = tf.graph_util.convert_variables_to_constants(sess, tmp_g, [n.name[:-2] for n in output_tensors])\n",
    "                dtypes = [n.dtype for n in input_tensors]\n",
    "                logger.info('optimize...')\n",
    "                tmp_g = optimize_for_inference(\n",
    "                    tmp_g,\n",
    "                    [n.name[:-2] for n in input_tensors],\n",
    "                    [n.name[:-2] for n in output_tensors],\n",
    "                    [dtype.as_datatype_enum for dtype in dtypes],\n",
    "                    False)\n",
    "            tmp_file = tempfile.NamedTemporaryFile('w', delete=False).name\n",
    "            logger.info('write graph to a tmp file: %s' % tmp_file)\n",
    "            with tf.gfile.GFile(tmp_file, 'wb') as f:\n",
    "                f.write(tmp_g.SerializeToString())\n",
    "            return tmp_file\n",
    "        except Exception as e:\n",
    "            logger.error('fail to optimize the graph!')\n",
    "            logger.error(e)\n",
    "\n",
    "\n",
    "    def get_estimator(self):\n",
    "        from tensorflow.python.estimator.estimator import Estimator\n",
    "        from tensorflow.python.estimator.run_config import RunConfig\n",
    "        from tensorflow.python.estimator.model_fn import EstimatorSpec\n",
    "\n",
    "        def model_fn(features, labels, mode, params):\n",
    "            with tf.gfile.GFile(self.graph_path, 'rb') as f:\n",
    "                graph_def = tf.GraphDef()\n",
    "                graph_def.ParseFromString(f.read())\n",
    "\n",
    "            input_names = ['input_ids', 'input_mask', 'input_type_ids']\n",
    "\n",
    "            output = tf.import_graph_def(graph_def,\n",
    "                                         input_map={k + ':0': features[k] for k in input_names},\n",
    "                                         return_elements=['final_encodes:0'])\n",
    "\n",
    "            return EstimatorSpec(mode=mode, predictions={'encodes': output[0]})\n",
    "\n",
    "        config = tf.ConfigProto(device_count={'GPU': 0})# if self.device_id < 0 else 1})\n",
    "#         config.gpu_options.allow_growth = True\n",
    "#         config.gpu_options.per_process_gpu_memory_fraction = self.gpu_memory_fraction\n",
    "        config.log_device_placement = False\n",
    "        # session-wise XLA doesn't seem to work on tf 1.10\n",
    "        # if args.xla:\n",
    "        #     config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "\n",
    "        return Estimator(model_fn=model_fn, config=RunConfig(session_config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length):\n",
    "    \n",
    "    def gen():       \n",
    "        for f in enumerate(features):\n",
    "            yield f\n",
    "\n",
    "    prefetch_size = 0 #args.prefetch_size if self.device_id > 0 else 0  # set to zero for CPU-worker\n",
    "    def input_fn():\n",
    "        return (tf.data.Dataset.from_generator(\n",
    "            gen,\n",
    "            output_types={'unique_ids':tf.int32,\n",
    "                          'input_ids': tf.int32,\n",
    "                          'input_mask': tf.int32,\n",
    "                          'input_type_ids': tf.int32,\n",
    "                          },\n",
    "            output_shapes={              \n",
    "                'unique_ids':(None, seq_length),\n",
    "                'input_ids': (None, seq_length),\n",
    "                'input_mask': (None, seq_length),\n",
    "                'input_type_ids': (None, seq_length)}).prefetch(prefetch_size))\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length):\n",
    "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "    all_unique_ids = []\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_input_type_ids = []\n",
    "\n",
    "    for feature in features:\n",
    "        all_unique_ids.append(feature.unique_id)\n",
    "        all_input_ids.append(feature.input_ids)\n",
    "        all_input_mask.append(feature.input_mask)\n",
    "        all_input_type_ids.append(feature.input_type_ids)\n",
    "\n",
    "    def input_fn(params):\n",
    "        \"\"\"The actual input function.\"\"\"\n",
    "        batch_size = 10\n",
    "\n",
    "        num_examples = len(features)\n",
    "\n",
    "        # This is for demo purposes and does NOT scale to large data sets. We do\n",
    "        # not use Dataset.from_generator() because that uses tf.py_func which is\n",
    "        # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "        d = tf.data.Dataset.from_tensor_slices({\n",
    "            \"unique_ids\": tf.constant(all_unique_ids, shape=[num_examples], dtype=tf.int32),\n",
    "            \"input_ids\": tf.constant(all_input_ids, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "            \"input_mask\": tf.constant(all_input_mask, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "            \"input_type_ids\": tf.constant(all_input_type_ids, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "        })\n",
    "        tf.logging.info(\"***SASHA: input_fn called ***\")\n",
    "        d = d.batch(batch_size=batch_size, drop_remainder=False)\n",
    "        return d\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SASHA: optimize_graph\n",
      "SASHA: init_checkpoint: ./uncased_L-24_H-1024_A-16/bert_model.ckpt\n",
      "INFO:tensorflow:Froze 373 variables.\n",
      "INFO:tensorflow:Converted 373 variables to const ops.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/xn/dh4vn2gx4x36vh0x051kpw1m0000gn/T/tmpjy91nbhf\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/xn/dh4vn2gx4x36vh0x051kpw1m0000gn/T/tmpjy91nbhf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {\n",
      "  key: \"GPU\"\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13f4b5cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function BertEstimator.get_estimator.<locals>.model_fn at 0x13dd93ea0>) includes params argument, but params are not passed to Estimator.\n"
     ]
    }
   ],
   "source": [
    "estimator = BertEstimator(args).get_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 0\n",
      "INFO:tensorflow:tokens: [CLS] jeans [SEP]\n",
      "INFO:tensorflow:input_ids: 101 6312 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "messages = [\"jeans\", \"denim\", \"pants\", \"trousers\",\n",
    "            \"package\", \"parcel\", \n",
    "            \"hot\", \"cold\", \n",
    "            \"I do not like you\", \"I love you\",\n",
    "            \"The package didn't arrived, what happens with it\", \"I havn't received my order yet\", \"My tracking number doesn't work. Can you help?\"]\n",
    "examples = list(map(lambda i_txt: InputExample(i_txt[0],i_txt[1],None) ,enumerate(messages)))\n",
    "features = convert_examples_to_features(examples=examples, seq_length=args.max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn = input_fn_builder(features=features, seq_length=args.max_seq_len)\n",
    "results_gen = estimator.predict(input_fn, yield_single_examples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find trained model in model_dir: /var/folders/xn/dh4vn2gx4x36vh0x051kpw1m0000gn/T/tmp8gbav1b5, running initialization to predict.\n",
      "INFO:tensorflow:***SASHA: input_fn called ***\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested return tensor 'final_encodes:0' not found in graph def",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/opt/pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    417\u001b[0m         results = c_api.TF_GraphImportGraphDefWithResults(\n\u001b[0;32m--> 418\u001b[0;31m             graph._c_graph, serialized, options)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScopedTFImportGraphDefResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Requested return tensor 'final_encodes:0' not found in graph def",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-914b90638d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    575\u001b[0m             input_fn, model_fn_lib.ModeKeys.PREDICT)\n\u001b[1;32m    576\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 577\u001b[0;31m             features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;31m# Call to warm_start has to be after model_fn is called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-2b0a007ab4de>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    135\u001b[0m             output = tf.import_graph_def(graph_def,\n\u001b[1;32m    136\u001b[0m                                          \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                                          return_elements=['final_encodes:0'])\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mEstimatorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'encodes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/opt/pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    420\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;31m# Create _DefinedFunctions for any imported functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Requested return tensor 'final_encodes:0' not found in graph def"
     ]
    }
   ],
   "source": [
    "results = list(results_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find trained model in model_dir: /var/folders/xn/dh4vn2gx4x36vh0x051kpw1m0000gn/T/tmp8gbav1b5, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested return tensor 'final_encodes:0' not found in graph def",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/opt/pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    417\u001b[0m         results = c_api.TF_GraphImportGraphDefWithResults(\n\u001b[0;32m--> 418\u001b[0;31m             graph._c_graph, serialized, options)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScopedTFImportGraphDefResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Requested return tensor 'final_encodes:0' not found in graph def",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-914b90638d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    575\u001b[0m             input_fn, model_fn_lib.ModeKeys.PREDICT)\n\u001b[1;32m    576\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 577\u001b[0;31m             features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;31m# Call to warm_start has to be after model_fn is called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-2b0a007ab4de>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    135\u001b[0m             output = tf.import_graph_def(graph_def,\n\u001b[1;32m    136\u001b[0m                                          \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                                          return_elements=['final_encodes:0'])\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mEstimatorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'encodes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/opt/pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    420\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;31m# Create _DefinedFunctions for any imported functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Requested return tensor 'final_encodes:0' not found in graph def"
     ]
    }
   ],
   "source": [
    "results = list(results_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[12]['layer_output_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features[10]['token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[12]['layer_output_6'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_id_to_feature[12].input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features[0]['layers'][2].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
